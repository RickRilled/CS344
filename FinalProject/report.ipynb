{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "I’ve been an improvisational comedian for 6 years, 4 of those professionally. Making people laugh has a special place in my heart. Therefore I follow a lot of comedic accounts on twitter. One night a tweet from 2013 popped up on my feed. It was from comedic blogger Keaton Patti, writer for YouTube channel Collegehumor. The tweet said, “I forced a bot to watch over 1,000 hours of Olive Garden commercials and then asked it to write an Olive Garden commercial of its own. Here is the first page.”  Might I say, the script didn’t disappoint. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](Images/Script.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I had seen this when it was posted quite some time ago, thought it was funny, and moved on. But as I studied AI, it became clear that there is no way a machine wrote this. It does not meander from topic to topic as a machine produced text sample would, and has running jokes that imply somebody crafted this (See the joke about friend 4). Janelle Shane replies in depth with how this is clearly inaccurate.   Patti made no indication that they wrote this, but continued to make more posts about more robots watching more bizarre content, then trying their hand at creating a new one. As a comedian and computer science undergrad, this made me rather upset. This is clearly a joke that actively lies about the nature of computers. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vision:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have spent a lot of time in this class talking about the nature of AI. Specifically, whether or not an AI “thinks” like a human does. Can AI ever become self-aware? Does it have a consciousness? What separates the machine from the man? These are all important metaphysical questions to be asking. As I have been studying this subject, in conjunction with philosophy of ethics and a computer science capstone, I began to ponder what those separations are. As I worked and pondered things like Patti’s tweet, one thing kept coming to mind: can a computer be funny?\n",
    "The vision for this project is to shed a light on what can be done with neural networks in the creative field. This project will also serve as a dive into if computers can make jokes and express the human trait of humor. It will also serve as a possible source of inspiration for creative people. Having a bunch of garbage jokes being thrown at you can actually be a great way to make new content.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Background:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project relies heavily on a neural network concept called LSTMs. These are essentially better RNNs that have a kind of pseudo memory. Basically, the output of the previous gradient step influences the next one. This is great for solving problems like sequencing, where we need to make predictions about what will come next based on what has already arrived. Most of the technologies used come from Chollet’s notebooks on Keras. Using that on top of tensorflow.  I will also use a technology called vectorization. I will put the characters in a one-hot encoded vector x, and the target words, the ones that come directly after them, in a label vector y. This will be what I feed the network. I also plan to use a concept called temperature. Essentially, when we have a character and we want to predict what the next word should be, temperature is what we reweight the distribution to. Higher temperatures mean wackier guesses. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First the data is loaded in by character. Then the characters are one-hot encoded, along with a target vector for predicted outcomes. Then the characters get vectorized so they can be fed into the model. The model is one LSTM layer with a dense output layer. Using a generated seed, we combine our predicted text with the training data to predict what each next character will be. Temperature takes guesses at different levels of distribution to choose the next character. Lower temperatures tend to be very repetitive, while higher temperatures are almost complete gibberish. Before hitting run, it is worth noting I attempted to train the network with individual words instead of characters. I was quickly disappeared from doing this. Nobody online has had any success with this, and a gentle nudge from my professor confirmed to stick with characters. I’ve also spent a good bit of time balancing the hyperparameters of the model and found these to produce the best results. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s take a look at some of the jokes it produced. Stop me if you’ve heard these before.\n",
    "\n",
    "it’s you.\n",
    "\n",
    "if you din’t look way to the drild, bay ry everun wis belions i lot money and be in phing in the say all tried.\n",
    "\n",
    "i when the street they are eately.\n",
    "\n",
    "if you smot farther sand.\n",
    "\n",
    "i the pensi.\n",
    "\n",
    "if go thing, but it to a discrete my, the refund.\n",
    "\n",
    "stupid the same theming who have a plane,\n",
    "\n",
    "if you i have a recal.\n",
    "\n",
    "\n",
    "\tAnd some words that repeatedly came up.\n",
    "\n",
    "Words:\n",
    "Men\n",
    "Sex\n",
    "Trouble\n",
    "Discrete \n",
    "Efficiency\n",
    "Stupid\n",
    "Plane \n",
    "\n",
    "Well I have to say I’m not surprised. The data looks similar to what Chollet was getting with his discount philosopher, even with tweaks to the learning rate and new data. See also how the loss starts low at the beginning of each epoch, then slowly climbs. This could be from the nature of the data, or having a dataset that’s too small. It started getting good at writing full sentences, but would lose steam when creating larger phrases. The only time I laughed was at the expensive of the robot trying so hard. Thankfully computers don’t have feelings so we can be honest and say he won’t win any open mic competitions anytime soon. However, the goal was imitation, and that is close to what we got. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implications:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are computers funny? As a comedian, I understand that what makes a joke funny is its relationship to these two factors: Truth and Surprise. The best jokes are when the subject honestly talks about their experiences and how it affects them, while also putting a surprising twist on things that nobody else has thought of yet. Humor is also a response to misery. Almost every joke is at the expense of some bad thing happening to someone. For these reasons, I think humor is a distinctly human trait. A computer can only do exactly what it is told to do. How can a computer create a surprising situation when it only can follow instructions? One of the jokes in the dataset actually puts this really nicely: “Artificial intelligence is no match for natural stupidity.” My immediate inclination is to propose an element of randomness, but randomness is not funny. There is no truth in randomness. Taking 3 random words and putting them together doesn’t make a funny sentence, the context surrounding them make them funny. \n",
    "\tThe ethical implications of a computer writing jokes are also interesting to me. If a computer writes an original joke that no one claims to have seen before, who gets the credit? Is it the designer of the program? Or does the computer get the credit? How do you even credit a computer? Is it the algorithm? Does the computer get its own stand up special? To me it seems obvious that the designer of the program should get the rights to whatever the machine makes. Because humor is such a distinct human trait, attributting creativeness to it would undermine what is actually happening with the person.\n",
    "\tI believe that computers are great tools for work and inspiration. They are good at imitating creative content. They cannot truly create from nothing. Keaton Patti was having fun when they wrote a script about Olive Garden, but subsequently brought up what it means to be human and why we are separate from machines. As artificial intelligence advances, we must hold onto the things that make us human. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bibliography:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tweets: \n",
    "[Keaton Patti](https://twitter.com/KeatonPatti/status/1006961202998726665)\n",
    "[Janelle Shane](https://twitter.com/JanelleCShane/status/1007061610005794817)\n",
    "\n",
    "LSTMs:\n",
    "[Understanding LSTMs](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "\n",
    "Chollet:\n",
    "[Keras](https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/8.1-text-generation-with-lstm.ipynb)\n",
    "\n",
    "Humor and Computers, New York Times:\n",
    "[Can Computers Be Funny?](https://www.nytimes.com/2013/01/06/opinion/sunday/can-computers-be-funny.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
