{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "On June 13th, 2018, Keaton Patti, a comedic writer blogger, posted this tweet:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](Images/Keaton.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script produced looked like this:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](Images/Script.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After studying neural networks and text generation, it becomes fairly obvious that a bot did not write this. There are jokes with setups and punchlines, which doesn’t match a networks typical meandering and inability to reference itself. As he is a comedic blogger, I would not be shocked if he just wrote this like a joke. When I discovered this, I was quite upset. There is nothing wrong with that. Claiming that you really fed information to a bot falsely represents what neural nets can do, and by extension, your own comedic process. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For these reasons, I want to create my own bot to write comedy scripts for my final project.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project will rely heavily on a LSTM network. This is a network that has a type of pseudo memory. It uses information from the previous node in a network to affect the weights and biases of the current node. I will use a Keras implementation, which is built on top of google TensorFlow, which allows for easy creation of layers, and a lot of activation/loss functions to play around with.  The scripts the network will be trained on come from the IMDB script database, available publically online.  I will use tokenized word objects from these scripts in a word enneagram. I will also use a technology called vectorization. I will put the words in a one-hot encoded vector x, and the target words, the ones that come directly after them, in a label vector y. This will be what I feed the network. I also plan to use a concept called temperature. Essentially, when we have a word and we want to predict what the next word should be, temperature is what we reweight the distribution to.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the script will be loaded into Python as a static file, read into a text variable. I will then create a list of sentences that are in the script. Each word will then get tokenized and sorted to count the number of unique characters. Next I will vectorize the text as described above. The model will built after, and I expect to use at least two layers, one LSTM and one dense, with a softmax activation function. Loss will be calculated with categorical cross entropy, provided by Keras. For each epoch, the network will be fit with the vectors and based on a random sample of words pulled from scripts. The model can predict what it thinks should come next and update the text it is generation from. This predictive text will be the final product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results will be “original” scripts written by a computer. While this is similar to robots that write Shakespeare or other writers, I am not trying to write like any particular person. I want to create original work that could resemble a movie script. This is also not meant to fool anyone. The results are to be inspirational to creative people who are in a rut. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is an argument to be made about the ethical implications of when the writing is directly based on another original IP. What if the robot commits plagiarism? Who can be blamed? What if it actually learns how to make jokes, but makes the same joke as one in “Scott Pilgrim”. Are jokes original IP?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
